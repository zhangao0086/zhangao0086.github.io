---
layout: post
title: "Ferret-UI 2 学习笔记"
date: 2025-01-20 21:33:21 +0800
categories: [分享]
article_type: 1
typora-root-url: ../../github.io
---

![](/assets/img/ferret-ui-2-caption.jpg)

> 很多爱心~

# 它是什么

> Ferret-UI 2: Mastering Universal User Interface Understanding Across Platforms
>
> https://arxiv.org/abs/2410.18967

![](/assets/img/ferret-ui-2-1.png)

它是苹果推出的跨平台 UI 理解模型，相比第一版 Ferret-UI，这一版本有三大创新：

1. 具有平台多样性（iPhone、Android、iPad、Webpage、AppleTV）、分辨率多样性（各种屏幕 size）的特点
2. 通过 Adaptive Gridding 优化了高分辨率图像理解能力
3. 用 gpt-4o 生成高质量的多模态训练数据

以此执行复杂的、以用户为中心的交互，覆盖 5 个平台 * 9 种任务类型。

为了更好的理解 Ferret-UI 2，可以看看 Ferret-UI 的限制有哪些：

1. 固定的分辨率：336x672 or 672x336
2. 单一平台支持：只有移动端，iPhone and Android
3. 训练数据质量不高：主要依赖基于文本的 gpt-4，bbox 仅以文本形式描述，缺乏 vision 输入和 UI 元素之间的空间关系
4. 交互模型存在局限性：依赖用户指令而非意图，导致体验不佳；同时只支持单步交互
5. 泛化能力不行：在不同平台和不同类型的 UI 任务上的泛化能力较弱，性能不高

Ferret-UI 2 显著改进了这些局限性。

# 业界相关工作

## Single-Platform UI Agents

聚焦在特定平台生态的任务自动化，如：

- Android: AppAgent、AutoDroid、MobileFlow 等
- Web: WebShop、WebAgent、AutoWebGLM 等

还有聚焦在 web search 上的 MindSearch；聚焦在复杂的计算机 OS 操作的 AssistGUI、OS-Copilot、UFO 等等。

它们的工作显著改善了 task-specific 自动化能力，但它们的单平台限制了跨平台场景的灵活性。

## Multi-Platform UI Agents

为了解决日益复杂的平台生态、多样性问题（新设备层出不穷，如鸿蒙），又出现了一批选手：

- OmniACT 支持桌面端和 Web 端
- CogAgent 在 PC 网页和 Android 设备上支持 UI 导航
- Mind2Web 和 Mobile-Agent 支持跨平台无缝操作，Mobile-Agent V2 更进一步，支持鸿蒙、Android 平台下，英语和非英语场景
- Ferret-UI 聚焦在 Android 和 iPhone 平台，使用 MLLM 解决指代和定位问题

它们旨在多种设备类型上，执行更复杂、基于用户意图的交互，为真正通用的多模态 Agents 铺平道路。

## Ferret-UI 2 的优势

与上述工作相比，Ferret-UI 2 是第一个针对跨不同平台（智能手机、平板电脑、网页和智能电视）的通用 UI 理解模型。它专注于 fine-grained referring、grounding 和 reasoning，旨在创建一个适用于不同用户界面操作的通用 Agent。

# 关于数据集

数据集分类和作用：

![](/assets/img/ferret-ui-2-2.png)

## Core-set 数据集

整体的生成流程：

![](/assets/img/ferret-ui-2-3.png)

先从 Raw Annotation Collection 开始，由不同平台的数据集构成，根据平台类型用不同的标注策略：

- iPhone，iPad 和 AppleTV：在不同的使用场景场景收集人工标注的数据，包括控件 bbox 坐标和标签。为了节省标注成本，文本的 bbox 是用 OCR 检测到的文本和边框标注的，0.5 的 OCR 置信度
- 网页：来自 WebUI 数据集，对 UI 控件和非图片元素，bbox 直接从原始 HTML 树中解析而得；图片元素则是进一步通过 OCR 检测而来
- Android：Android 的屏幕截图、bbox 和文字标注转化自 RICO 数据集，和 WebUI 数据集相似，对图片元素执行了 OCR 以补充缺失的文本标注

所有数据都经过了相同的过滤规则：

1. 过滤或者缩小超过屏幕范围的 bbox，然后删除过滤后为空的屏幕截图
2. 因为 Ferret-UI 2 模型不打算支持多语，所以对文本注解中，ASCII 字符不超过 5% 的进行了过滤

然后再将 bbox 和 UI 标签关职在一起，去除和 UI 相关性比较低的，将剩余的标签分为 13 类：Checkbox、Button、Container、Dialog、Icon、PageControl、Picture、SegmentedControl、Slider、TabBar、Text、TextField 和 Toggle。

这就是 Core-set 数据集，它将被用来构建基础和高级任务的数据。

此外为了解决 iPad、AppleTV 平台屏幕截图少导致数据分布不均匀的问题，引入了两项额外的策略：

1. 在训练时为不同的平台分配不同的损失权重
2. 为 iPad、AppleTV 的每个示例生成三种类型的高级任务，而其他平台只生成一种

对比 Ferret-UI 的训练数据集，Ferret-UI 依赖模型进行 bbox 检测，而 Ferret-UI 2 的训练数据集主要利用人工采集或从 HTML 中解析，这种方式显基提高了标注质量。

## 生成任务数据

任务分为两种类型：基础任务、高级任务。

基础任务就是上图 Core-set 流程中的 3 个指代任务和 3 个定位任务：

- 指代任务
  1. OCR：识别给定文字的 bbox
  2. 控制分类：预测元素的 UI 控件类型
  3. 可点击性：预测选择的控件是否能响应点击
- 定位任务
  1. 列出控件列表：列出屏幕中所有的控件
  2. 定位文本：找到给定文本的位置
  3. 定位控件：根据给定的描述找到控件

高级任务则是通过 gpt-4o 生成，通过 prompt 把屏幕截图上的 bbox 告诉 gpt-4o，让 gpt-4o 生成一组和 UI 控件相关的 QA 任务。和 Ferret-UI 不同，后者没有屏幕截图这种图像信息，仅有纯文本 prompts，因此 Ferret-UI 2 对 UI 控件之间的关系理解能力更强。高级任务也有 3 种：

1. 全面描述：描述屏幕的全局和局部功能信息
2. 多轮感知问答：基于 UI 感知能力进行多轮问答
3. 多办交互问答：基于当前屏幕状态进行单步和面向用户的多轮 UI 交互问答

但就论文作者的经验来看，gpt-4o 如果仅有原始的屏幕截图信息还是很难找到 UI 控件的位置，为了解决该问题，在生成多轮感知 & 交互问答的训练样本时，引入了 SoM visual prompting。

## 什么是 Set-of-Mask (SoM) visual prompting

示例一：

1. 在图片输入上，每个 UI 控件都标注了 bbox 和唯一 ID
2. 相同的 UI 控件类型用使用相同的颜色标注

![](/assets/img/ferret-ui-2-4.png)

示例二，包含其他平台的例子：

![](/assets/img/ferret-ui-2-5.png)

# 模型架构

![](/assets/img/ferret-ui-2-6.png)

Ferret-UI 2 建立在 Ferret-UI 的基础上，通过 AnyRes 方法增强指代和定位能力，使 CLIP 图像编码器能更有效地处理不同分辨率的图像，从而提高模型的泛化能力。

## 什么是 Adaptive Gridding

它是 Ferret-UI 2 模型中的一项关键技术创新，用于处理高分辨率图像编码，以实现对 UI 屏幕的精确理解和交互，可以把它理解为 Ferret-UI 2 的切图规则。之所以叫自适应，是旨在给定的推理成本（内存和算力、推理时间等）限制下，找到最优的网格，以最小化图像分辨率的失真。

整个算法分为三步：

1. 动态调整网格大小：算法动态调整网格的行数 Nw 和列数 Nh，在保持图像信息的同时，减少网格数量。如此一来，对一个高分辨率的 UI 截图来说，算法可能会选择一个较小的网格数量，网格数据少就意味着计算成本小
2. 最小化失真：算法通过计算纵横比变化和相对像素变化，找到最优的网格配置，使得图像的失真最小。这确保了在减少网格数量的同时，图像信息的损失最小化
3. 大小限制：算法设置了一个大小限制 N，确保总网格数量不超过 floor(N^2 / 4)，从而在计算资源和图像信息保留之间找到一个平衡

与无限制的 AnyRes 相比，自适应网格独特之处是平衡了推理成本和分辨率精度，根据 Ferret-UI 2 的设计，N 为 8，所以最多有 16 个网格。
