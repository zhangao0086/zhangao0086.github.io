---
layout: post
title: "LRM 在编程中大放异彩"
date: 2025-02-23 23:19:29 +0800
categories: [分享]
article_type: 1
typora-root-url: ../../github.io
---

> Competitive Programming with Large Reasoning Models
>
> https://arxiv.org/abs/2502.06807

# 从 o1 到 o3，OpenAI 的进阶之路

OpenAI 的 o3 模型在编程领域又掀起了一阵热潮，要理解 o3，咱们得先从它的 “前辈” o1 说起。OpenAI 的 o1 和 deepseek 一样，是通过强化学习来训练的大型语言模型，专门用于攻克复杂的推理任务。就好比一个聪明的解题高手，面对难题时，它会像人类一样，有条不紊地一步步思考，在给出回答之前，先在内部生成一系列连贯的思路，也就是所谓的 “思维链”。强化学习就像是给这个高手配备了一个 “纠错教练”，帮助它不断找出并改正错误，把复杂的任务拆分成一个个小任务，当一种方法行不通时，还能探索其他的解决方案。不此如此，o1 还学会了使用外部工具，能在安全的环境中编写和执行代码，让它在编程领域更加得心应手。

在 CodeForces 这个国际编程竞赛网站上，o1 的表现堪称惊艳。它和非推理的 LLM（如 gpt-4o）以及早期的推理模型 o1-preview 相比，成绩遥遥领先：

![](/assets/img/lrm-in-coding-1.png)

o1-preview 的 CodeForces 评级达到了 1258（超过了 62% 的对手），而 gpt-4o 只有 808（11%），而进一步训练的 o1，评级飙升到了 1673（89%），基本上是 AI 编程领域的一个里程碑。

其后，为了在 2024 国际信息学奥林匹克竞赛（IOI）中取得好成绩，OpenAI 又基于 o1 打造了 o1-ioi，不仅在强化学习阶段针对编码任务进行了更多的训练，还加入了专门为编程竞赛设计的测试时推理策略（test-time inference strategies），就像一个原本已经很厉害的运动员，又针对特定比赛项目进行了专项训练、掌握了独特的比赛策略，在 ioi 评分上自然更上一层楼。

在 CodeForces 竞赛模拟中，o1-ioi 的表现非常出色，它的 CodeForces 成绩达到了 1807（93%）。采用简单的过滤策略后，成绩更是上升到了 2092（96%），搭配完整的测试时策略（test-time strategy）后达到了 2214（98%）。在 2024 IOI 现场竞赛中也有不错的表现，虽然在正常提交限制下，它的得分是 213 分，处于 49%，但当提交限制放宽到每个问题 10000 次时，它的成绩大幅提升、达到了 362.14 分，超过了金牌门槛：

![](/assets/img/lrm-in-coding-2.png)

以上是背景。

OpenAI 并没有满足于此，他们继续探索强化学习的极限，于是 o3 诞生了。o3 和 o1-ioi 最大的不同在于，**它不依赖人工设计的测试时策略**，而是能够自主开发和执行自己的测试时推理策略（test-time **reasoning** strategies）。还是用运动员的例子，只是这次运动员不再依赖教练制定的固定策略，而是能够根据比赛时的实际情况，自己灵活地调整战术，这种自主性让 o3 在编程能力上有了质的飞跃。

# o3 在竞赛中的表现

在 CodeForces 基准测试中，o3 达到了 2724（99.8%），相比 o1-ioi 的 2214（98%）有了大幅提升，这意味着它在解决复杂算法问题时，可靠性更高、能够解决的问题范围更广，接近人类顶尖对手的水平了：

![](/assets/img/lrm-in-coding-3.png)

在 2024 IOI 基准测试中，o3 同样表现出色，每个问题最多只能提交 50 次的情况下，o3 的得分达到了 395.64 分，超过了金牌门槛：

![](/assets/img/lrm-in-coding-4.png)

> o1-ioi 在同样的提交次数限制下只有 213 分

o3 的采样和选择与 o1-ioi 有很大不同：

1. **强化学习驱动，无需人工策略**：它对每个问题采样 1k 个解决方案，然后根据执行结果和性能指标来选择最优的 top50，尽管策略看似简单，但它能够覆盖很多甚至所有的子任务
2. **强大的推理能力**：o3 能够理解复杂的问题描述，并将其转化为有效的代码，它可以通过分析问题的条件、约束和目标，运用已有的知识和经验，逐步推导出解决方案。比如处理数学问题时，它能够运用逻辑推理和数学原理，最终编写代码来求解
3. **自主学习能力**：o3 可以在不断的训练和实践中积累经验、提升自己的编程水平，随着时间的推移，训练数据不断丰富，模型接触到更多类型的问题和解决方案，它就能够解决越来越复杂的问题，并且在面对新的问题时，能够快速适应并找到解决方案

论文中还特意提到了下面这段代码：

![](/assets/img/lrm-in-coding-5.png)

o3 模型对于一些验证难度较大的问题，它会先编写简单的暴力解法，以确保结果的正确性，然后再与更优化的算法实现进行交叉检查，这种自我验证机制大大提高了 o3 模型解决方案的可靠性。拿上述代码来说，test_random_small 测试函数用于对 solve_main 函数进行随机小规模测试，整体逻辑是生成随机字符串，调用暴力解法得到正确结果，再调用被测试函数进行对比，若不一致则输出错误信息并返回 False，全部通过则返回 True。

在实际应用中，这一策略带来的作用很明显：

1. **提高了编程效率**：o3 能够高效解决复杂问题，特别是在处理大型项目和复杂算法时，它可以在短时间内提供高质量的、可靠的解决方案，减少开发周期
2. **对项目的适应性强**：因为无需依赖特定策略，这使它在面对不同类型的问题时，具有很强的适应性和灵活性

# o3 在实际应用中的表现

除了在竞争编程中的表现，o3 在实际的软件工程任务中也展现出了强大的实力。

HackerRank Astra 数据集包含 65 个面向真实项目的编码挑战，模拟了真实世界的软件开发任务，涵盖了 React.js、Django 和 Node.js 等多种框架。o1-preview 模型在这个数据集上就已经展现出了一定的优势，相比 gpt-4o，它在首次通过率（pass@1）上提高了 9.98%，平均得分提高了 6.03 分：

![](/assets/img/lrm-in-coding-6.png)

而经过强化学习进一步微调的 o1 模型，首次通过率达到了 63.92%，平均得分达到 75.80%，比 o1-preview 又提高了 3.03%。

SWE-Bench Verified 是 OpenAI 用于更可靠地评估 AI 模型解决实际软件问题能力的数据集。在这个数据集上，o1-preview 相比 gpt-4o 有 8.1% 的性能提升，而 o1 通过增加强化学习计算量，又取得了 7.6% 的进一步提升；o3 更是凭借显著更多的计算资源训练，比 o1 实现了 22.8% 的惊人提升：

![](/assets/img/lrm-in-coding-7.png)

这些结果充分表明，o3 的增强推理能力不仅适用于竞争编程，在实际的软件工程任务中同样具有极高的应用价值。

# 对未来编程的深远影响

传统编程，就像是工匠们精心雕琢每一个零件，程序员们需要一行行地编写代码、构建复杂的逻辑结构。在这个过程中，一个小错误可能就需要花费大量时间去排查和修复，哪怕是开发一个简单的网站，从前端页面的布局到后端数据的处理，都需要程序员具备丰富的知识和经验，而随着 AI 技术的不断进步，o3 这类 reasoning 模型带来了全新的思路，它们借助强化学习和 LLM，让编程变得更加智能和高效。对于一些常见的功能模块，它们可以迅速生成代码框架，开发人员只需在此基础上进行一些个性化的修改就能完成开发任务，大大缩短交付周期；在面对一些复杂的算法实现时，reasoning 模型也能提供准确可靠的代码实现，帮助开发人员解决难题。

无论是专业程序员还是对 AI 充满好奇的爱好者，都值得持续关注 reasoning 模型的发展动态，共同见证人工智能编程新时代的到来。
